\documentclass{article}

\usepackage{project_440_540}
% Please submit it as is here, with line numbers.
% If you'd like a "less draft"-looking version for your website or something after:
%     \usepackage[final]{project_440_540}   % keeps the footer but kills line numbers,
%     \usepackage[preprint]{project_440_540}   % removes both


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts

\usepackage[USenglish]{babel}  % there's a "canadian" option, but it's an alias for USenglish,
                               % and for some reason it makes csqoutes behave differently...
\usepackage{csquotes}       % smarter handling of quotes (used by biblatex)

\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{tikz}
\usepackage{float}
\usepackage{hyperref}


% I recommend the biblatex package.
% If you hate it for some reason, though, you can use natbib instead:
% comment out this block and uncomment the next one.

\usepackage[style=authoryear,maxbibnames=30]{biblatex}
\addbibresource{refs.bib}
\renewbibmacro{in:}{}  % drops a silly "In:" from biblatex format
\let\citet\textcite    % I do this alias because I/others find it more familiar, idk.
\let\citep\parencite   % biblatex also has a natbib option to make these,
                       % but makes other changes I don't care about too.
\usepackage{bibentry}
%\usepackage{natbib}



\usepackage[capitalize,noabbrev]{cleveref}


\title{Implementing Complex Valued Neural Networks}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Alexander MacFarlane\\
  \texttt{djmacfarlanez@gmail.com}
}

\begin{document}
\maketitle


\begin{abstract}
  Popular neural network frameworks like PyTorch and Keras with TensorFlow provide limited support for CVNNs. In this paper, we present an implementation of CVNNs using custom layers and activation functions in Keras with TensorFlow, taking advantage of TensorFlow's support for complex tensors. We evaluate the implementation on several benchmark datasets, including MNIST, Fashion MNIST, and audio classification tasks. Our experiments show that CVNNs can achieve competitive performance with real-valued neural networks, while offering the potential for improved efficiency and applicability in domains where complex values are prevalent. \\ \\
  \textbf{GitHub:}\\ \url{https://github.com/DJMacFarlane/Complex-Valued-Neural-Nets}
\end{abstract}
\section{Introduction}
Complex valued neural networks (CVNNs) offer several potential advantages over real valued neural networks (RVNNs). By incorporating both phase and magnitude in each value, CVNNs allow for a richer representation of data. This increased information content in each input and parameter can lead to a reduction in the number of parameters, subsequently lowering the likelihood of exploding and vanishing gradients while also reducing the need for regularization. Furthermore, some types of data are naturally suited for representation using complex numbers.

CVNNs hold great promise in domains where complex values are already extensively utilized, such as quantum computing and signal processing. Outputs from Fourier transforms and other complex representations can be directly fed into the network, eliminating the need to separate or remove information from each value as required with RVNNs. Additionally, certain complex transforms and filters can be applied to images, thereby reducing the need for convolutions in image classification tasks\footnote{See \cite{ko2022coshnet}}.

Despite these advantages, many popular neural network frameworks, such as PyTorch and Keras with TensorFlow, offer limited support for complex valued neural networks by default. Nevertheless, TensorFlow does provide support for complex tensors, which enables the implementation of CVNNs by defining custom layers. In this project, we take advantage of this functionality to explore the potential of CVNNs further.

\section{Related Works}
\begin{center}\textbf{\fullcite{CVNN_book}}\end{center}
This book provides an overview of complex valued neural networks and some applications. Much of the implementation is based on concepts from this author's works.\\
\begin{center}\textbf{\fullcite{ko2022coshnet}}\end{center}
In this paper they explore shearlets and CVNNs in image processing to reduce the need for convolution, increase efficiency and improve performance. This is an excellent demonstration that CVNNs are worth investigating. \\ \\
\begin{center}\textbf{\fullcite{biocvnn}}\end{center}
The primary motivation for this project was the potential of CVNNs to more accurately approximate biological networks. The paper investigates CVNNs, which are designed to be more similar to biological neural networks than their real-valued neural network (RVNN) counterparts, demonstrating superior performance in certain tasks. However, the paper's main drawback lies in its reliance on gradient descent as a training method, as this is likely an unrealistic learning mechanism for biological systems \citep{hintonforwardforward}.

\section{Description}
In order to implement complex valued neural networks we defined the following custom layers and activations for keras. Keras and TensorFlow do support Wirtinger derivatives so there is no need to modify the backpropagation methods. \\
\subsection{Layers}
\begin{enumerate}
  \item \textbf{ComplexDense}: A dense layer that takes complex or real inputs and outputs complex outputs.

  \item \textbf{ComplexConv2D}: A 2D complex convolution layer that takes complex or real inputs and outputs complex outputs.

  \item \textbf{ComplexConv1D}: A 1D complex convolution layer that takes complex or real inputs and outputs complex outputs.

  \item \textbf{ComplexDropout}: A complex dropout layer that takes complex inputs and performs dropout separately on the real and imaginary parts.

  \item \textbf{ComplexMaxPool2D}: A complex max-pooling layer that takes complex inputs and outputs complex outputs.

  \item \textbf{ComplexAvgPool2D}: A complex average-pooling layer that takes complex inputs and outputs complex outputs.

  \item \textbf{ComplexLayerNormalization}: A complex layer normalization layer that takes complex inputs and outputs complex outputs.

  \item \textbf{ComplexUpSampling2D}: A complex upsampling layer that takes complex inputs and outputs complex outputs.
\end{enumerate}

\subsection{Activations}
\begin{enumerate}
  \item \textbf{abs\_softmax(x)}: This function computes the softmax of the absolute values of the input tensor $x$. The softmax function is applied to the absolute values of the elements, normalizing them to create a probability distribution.

  \item \textbf{real\_softmax(x)}: This function computes the softmax of the real parts of the input tensor $x$, which has complex elements. The softmax function is applied to the real parts, normalizing them to create a probability distribution.

  \item \textbf{imag\_softmax(x)}: This function computes the softmax of the imaginary parts of the input tensor $x$, which has complex elements. The softmax function is applied to the imaginary parts, normalizing them to create a probability distribution.

  \item \textbf{polar\_softmax(x)}: This function computes the softmax of the angles (phases) of the input tensor $x$, which has complex elements. The softmax function is applied to the angles, normalizing them to create a probability distribution.

  \item \textbf{cmplx\_rrelu(x)}: This function applies the Rectified Linear Unit (ReLU) activation function only to the real parts of the input tensor $x$, which has complex elements. The imaginary parts are left unchanged.

  \item \textbf{cmplx\_crelu(x)}: This function applies the Rectified Linear Unit (ReLU) activation function to both the real and imaginary parts of the input tensor $x$, which has complex elements.

  \item \textbf{polar\_relu(x)}: This function applies the Rectified Linear Unit (ReLU) activation function to the magnitudes (absolute values) of the input tensor $x$, which has complex elements. The phases (angles) are left unchanged.
\end{enumerate}

\section{Experiments}
\subsection{MNIST}
To test the implementation we do a trial run on MNIST data set to verify the implementation is functioning. For the real neural network we use 32 filter 4 by 4 kernels with softmax output layer. The complex network has 16 filters of 4 by 4 kernels to halve the number of parameters. CVNNs do not offer much benefit in a simple case like this, but is a convenient test set.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{../figs/combined.png}
  \caption{As expected we get similar performance.}
\end{figure}
\newpage
\subsection{Fashion MNIST}
We also tested training on the 2D fourier transform of Fashion MNIST data set \citep{fashionmnist}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{../figs/fashionmnist.png}
  \caption{For complex case we used fourier transform values directly.}
\end{figure}

\subsection{Audio}
\subsubsection{Dog versus Cat Audio}
After training on images we downloaded a simple dataset of cat sounds vs dog sounds using complex valued outputs of \texttt{tf.signal.stft(waveform)} as inputs to a convolutional neural network.

 Data set here \url{https://www.kaggle.com/datasets/mmoreaux/audio-cats-and-dogs}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{../figs/cats_dogs_accuracy.png}
  \caption{Given the small size of the data set we overfit quickly.}
\end{figure}
\newpage
\subsubsection{Emotion Classification}
Using a subset of the Toronto emotional speech set \citep{TESS} we trained a complex valued neural network on the spectrogram (complex valued) to categorize audio into classes based on predicted emotion of the speaker.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{../figs/emotion_accuracy.png}
  \caption{We achieved very quick convergence.}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{../figs/confusion_matrix.png}
  \title{Confusion Matrix}
  \caption{The model sometimes confused happy with pleasant surprise.}
\end{figure}
\newpage
\section{Discussion}
Our experiments on various datasets, including MNIST, Fashion MNIST, and audio classification tasks, demonstrate the potential of CVNNs as a viable alternative to traditional RVNNs. In the MNIST experiment, we observed that the CVNNs with fewer parameters performed competitively with the RVNNs. Furthermore, when applying CVNNs directly to the 2D Fourier transform of the Fashion MNIST dataset, we achieved satisfactory results, showcasing the ability of CVNNs to handle complex input data effectively.

In the audio classification tasks, we tested CVNNs on both cat and dog sounds and emotion classification using a subset of the Toronto emotional speech set. In both cases, we observed rapid convergence and competitive performance. However, overfitting occurred quickly in the cat and dog audio dataset due to its small size. In the emotion classification task, we got excellent results.

The results of our experiments indicate that CVNNs can achieve competitive performance with RVNNs and, in some cases, provide improved efficiency and applicability in domains where complex values are prevalent. By implementing custom layers and activation functions in Keras with TensorFlow, we have taken advantage of TensorFlow's support for complex tensors, allowing us to explore CVNNs more in the future. Though we did not have enough time to experiment with complex valued outputs or complex transormers.

Future work on CVNNs may focus on optimizing the architecture, incorporating more sophisticated regularization techniques, and exploring additional applications in areas such as quantum computing, signal processing, and image processing. Additionally, it would be interesting to investigate the potential of CVNNs in approximating biological networks more accurately, as alluded to in the Biologically Inspired Complex-Valued Neural Networks paper \citep{biocvnn}. As complex numbers may allow for easier encoding of frequency and phase of spiking patterns of neurons.

\newpage
\printbibliography
\end{document}
